{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcd0160-26ba-4f21-8d53-4752b4d67505",
   "metadata": {},
   "source": [
    "# Домашнее задание №2\n",
    "\n",
    "SQL-инъекция (SQLi) представляет собой одну из наиболее распространенных и опасных уязвимостей веб-приложений. Она возникает, когда злоумышленник вставляет вредоносные SQL-запросы в поля ввода пользовательских данных. Эти запросы могут позволить злоумышленнику получить несанкционированный доступ к базе данных, извлечь, изменить или удалить данные, а в некоторых случаях даже получить контроль над сервером.\n",
    "\n",
    "Обнаружение и предотвращение SQL-инъекций крайне важно, поскольку такие атаки могут нанести значительный ущерб как бизнесу, так и пользователям. Традиционные методы защиты включают использование параметризованных запросов, подготовленных выражений и тщательную проверку и экранирование входных данных. Однако, несмотря на эти меры, уязвимости все же могут возникать из-за ошибок в программировании или человеческого фактора.\n",
    "\n",
    "С развитием технологий машинного обучения (ML) появились новые подходы к обнаружению и предотвращению SQL-инъекций. Методы машинного обучения позволяют анализировать большие объемы данных и выявлять аномалии или подозрительные шаблоны, указывающие на попытку внедрения вредоносного SQL-запроса. Системы, основанные на ML, могут обучаться на исторических данных и адаптироваться к новым видам атак, что делает их мощным инструментом для обеспечения безопасности веб-приложений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e343b69-a243-4aec-8dc6-79016551db69",
   "metadata": {},
   "source": [
    "## Подготовка виртуального окружения Python\n",
    "\n",
    "Предварительно нужно установить все зависимые модули. Все необходимые зависимости перечислены в requirements.txt файле. Чтобы установить зависимости, выполните в терминале следующую команду:\n",
    "\n",
    "python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12a69c-0142-4e26-b528-00203a73c8c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Описания задания\n",
    "\n",
    "В данном задании мы постараемся изучить возможность детектирования SQL-инъекций на основе подхода обработки естественного языка (NLP) к анализу данных. Современные методы NLP позволяют эффективно обрабатывать и анализировать текстовые данные, выявляя закономерности и аномалии, которые могут ускользнуть от традиционных методов защиты.\n",
    "\n",
    "Используя инструменты и методы NLP, мы будем анализировать текстовые входные данные веб-приложений, чаще всего передаваемый в полезной нагрузки прикладных протоколов, чтобы обнаружить подозрительные паттерны, характерные для SQL-инъекций. Это включает в себя такие техники, как токенизация (`RegexpTokenizer`), векторизация текста (`TfidfVectorizer`, `CountVectorizer`), предобработка данных (отброс несущественных токенов на основе анализа главных компонент),обучение моделей классификации.\n",
    "\n",
    "В рамках работы вам предоставлено два размеченных датасета (`train-dataset.tsv` и `test-dataset.csv`)  с полезной нагрузкой в виде текста и метки 0 (легитимный текст) и 1 (payload содержащий sql инъекцию) \n",
    "\n",
    "Для выполнения домашнего задания вам необходимо:\n",
    "1. Предустановить зависимости:\n",
    "```shell\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "2. Загрузить исходный датасет `train-dataset.tsv` при помощи модуля `pandas` и разбить его на обучающий и тестовый набор данных: \n",
    "```python\n",
    "source_df = pd.read_csv(\"datasets/train-dataset.tsv\", sep='\\t', engine='python')\n",
    "df_train, df_test = train_test_split(source_df, test_size=0.5)\n",
    "x_train, y_train = df_train['payload'].values, df_train['label'].values\n",
    "x_test, y_test = df_test['payload'].values, df_test['label'].values\n",
    "```\n",
    "\n",
    "3. Извлечь токенезировать и векторизировать данные представленные в `train-dataset.tsv` при помощи инструментов `nltk.tokenize.RegexpTokenizer`, `sklearn.feature_extraction.text.TfidfVectorizer`, `sklearn.feature_extraction.text.CountVectorizer`\n",
    "\n",
    "4. Выбрать ключевые, наиболее значемые токены при помощи инструментов `sklearn.feature_selection.SelectFromModel` и `sklearn.decomposition.TruncatedSVD`\n",
    "\n",
    "5. Обучить модель на выбранных ключевых признаках\n",
    "\n",
    "6. Протестировать обученную модель на тестовых данных из набора `train-dataset.tsv`\n",
    "\n",
    "7. Подсчитать accuracy, построить матрицу ошибок\n",
    "\n",
    "8. Загрузить тестовый датасет `test-dataset.tsv` при помощи модуля `pandas`:\n",
    "\n",
    "9. Протестировать предобученную модель на новом наборе данных\n",
    "\n",
    "10. Подсчитать accuracy, построить матрицу ошибок\n",
    "\n",
    "11. Рассмотреть различные модели классификации, а также различные способы векторизации и токенизации данных, сравнить полученные результаты между собой"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77dbd7-0e6e-41c5-aa0e-b7d123126e10",
   "metadata": {},
   "source": [
    "## Теоретический материал\n",
    "\n",
    "Процесс машинного обучения моделей классификации позволяющих обнаруживать SQL инъекции можно свести к следующим этапам: \n",
    "\n",
    "#### 1. Предварительная обработка данных\n",
    "\n",
    "Основная цель предварительной обработки данных — подготовить текстовые данные для дальнейшего анализа. Это включает следующие шаги:\n",
    "\n",
    "- Токенизация: Разбиение текста на отдельные слова или фразы (токены).\n",
    "- Очистка текста: Удаление лишних символов, знаков препинания, стоп-слов и приведение слов к нижнему регистру.\n",
    "- Лемматизация или стемминг: Приведение слов к их начальной или базовой форме для уменьшения количества уникальных слов.\n",
    "\n",
    "\n",
    "**RegexpTokenizer** (регулярный токенизатор) — это инструмент для разбиения текста на токены (слова или фразы) с использованием регулярных выражений. Регулярные выражения позволяют задавать сложные шаблоны для поиска и извлечения нужных частей текста.\n",
    "\n",
    "#### 2. Векторизация данных\n",
    "\n",
    "Наиболее простым споосбом векторизации является `CountVectorizer`:\n",
    "\n",
    "`CountVectorizer` — это инструмент для преобразования текста в числовые векторы путем подсчета количества вхождений каждого слова в документ. В результате каждый документ представляется как вектор, где каждая позиция соответствует определенному слову, а значение в позиции — количество вхождений этого слова в документе.\n",
    "\n",
    "Пример:\n",
    "\n",
    "Документ 1: \"SELECT * FROM users\"\n",
    "Документ 2: \"DROP TABLE users\"\n",
    "Векторизация: [1, 1, 1, 0, 0] и [0, 0, 0, 1, 1]\n",
    "где каждая позиция соответствует слову(токену) из словаря: [\"SELECT\", \"*\", \"FROM\", \"DROP\", \"TABLE\"].\n",
    "\n",
    "Другим способом векторизации является `TfidfVectorizer`:\n",
    "\n",
    "`TfidfVectorizer` (Term Frequency-Inverse Document Frequency Vectorizer) — это инструмент обработки текста, используемый для преобразования текстовых данных в числовые векторы на основе значимости слов. Этот метод учитывает как частоту появления слова в документе (term frequency, TF), так и обратную частоту появления слова во всех документах коллекции (inverse document frequency, IDF).\n",
    "\n",
    "- TF (term frequency): измеряет частоту появления слова в документе. Обычно рассчитывается как отношение количества вхождений слова к общему количеству слов в документе: tf(t, d) = f(t, d) / sum(f(t', d) for t' in d)\n",
    "- IDF (inverse document frequency): измеряет, насколько редким или распространенным является слово в коллекции документов. Рассчитывается как логарифм отношения общего количества документов к количеству документов, содержащих данное слово: idf(t) = log(N / df(t))\n",
    "\n",
    "- Итоговый вес токена: tf-idf(t, d) = tf(t, d) * log(N / df(t))\n",
    "\n",
    "#### 3. Отбор токенов (Отбор признаков)\n",
    "\n",
    "Отбор токенов (или признаков) является важным шагом, который позволяет уменьшить размерность данных, улучшить производительность модели и избежать переобучения. Для этого используются различные методы, включая статистические и алгоритмические подходы:\n",
    "\n",
    "- `SelectFromModel`: Метод для отбора признаков, основанный на важности признаков, которую определяет обученная модель. Например, можно использовать Lasso (линейная модель с L1-регуляризацией) для оценки важности признаков.\n",
    "\n",
    "- `TruncatedSVD`: Метод понижения размерности, который применим к разреженным матрицам, например, к тем, которые получены с помощью `CountVectorizer` или `TfidfVectorizer`. `TruncatedSVD` уменьшает размерность данных, сохраняя наиболее важную информацию.\n",
    "\n",
    "Также возможно комбинация данных методов, для повышения точности отбора признаков.\n",
    "\n",
    "#### 4. Обучение модели\n",
    "\n",
    "После векторизации текстовых данных их можно использовать для обучения моделей машинного обучения. Основные шаги включают:\n",
    "\n",
    "- Разделение данных: Разделение данных на тренировочные и тестовые наборы.\n",
    "- Выбор модели: Выбор алгоритма машинного обучения, такого как логистическая регрессия, случайный лес, метод опорных векторов (SVM) или нейронные сети.\n",
    "- Обучение модели: Обучение модели на тренировочных данных с использованием векторизованных текстов.\n",
    "- Оценка модели: Оценка точности и эффективности модели на тестовых данных с помощью метрик, таких как точность, полнота, F-мера.\n",
    "\n",
    "#### 5. Обнаружение SQL-инъекций\n",
    "\n",
    "После обучения модель можно использовать для классификации новых входных данных. При поступлении нового ввода текстовые данные проходят через этапы предварительной обработки и векторизации, после чего обученная модель определяет, является ли ввод подозрительным на наличие SQL-инъекций.\n",
    "Преимущества подхода на основе NLP\n",
    "\n",
    "- Адаптивность: Модели машинного обучения могут адаптироваться к новым типам атак, обучаясь на новых данных.\n",
    "- Автоматизация: Системы на основе NLP могут автоматически анализировать и классифицировать большое количество входных данных, что повышает эффективность обнаружения уязвимостей.\n",
    "- Точность: Использование методов векторизации, таких как TF-IDF, позволяет учитывать контекст и значимость слов, что улучшает точность классификации.\n",
    "\n",
    "Использование методов NLP и машинного обучения для обнаружения SQL-инъекций представляет собой современный и эффективный подход, способный значительно повысить уровень безопасности веб-приложений. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7affc-74df-4bff-a326-f1d5927d35ee",
   "metadata": {},
   "source": [
    "## Описание лабораторного стенда\n",
    "\n",
    "#### Описание `FeatureExtractor` \n",
    "`FeatureExtractor` — класс, отвечающий за извлечение признаков (токенизацию и векторизацию данных) из исходных датафреймов. Имеет два метода:\n",
    "\n",
    "`fit_extract` — анализирует предоставленные тексты и строит внутренние структуры данных, такие как словарь терминов (слов) и их IDF (inverse document frequency, обратная частота документа) значения. IDF вычисляется как логарифм отношения общего количества документов к количеству документов, содержащих данный термин, после чего преобразует текстовые данные в векторное представление на основе вычисленных векторов TF-IDF;\n",
    "\n",
    "`extract_features` — выполняет только преобразование входных текстовых данных в соответствующие вектора TF-IDF с использованием уже обученных внутренних структур, таких как словарь терминов и IDF значения.\n",
    "\n",
    "`FeatureExtractor` — поддерживает следующие режимы работы:\n",
    "\n",
    "* TF-IDF ('tf-idf'):\n",
    "\n",
    "    Описание: TF-IDF (Term Frequency-Inverse Document Frequency) — это метод вычисления веса слова в документе относительно его важности в коллекции документов.\n",
    "    Применение: Каждый документ представляется в виде вектора, где каждая компонента отражает вес термина (слова), вычисленный по формуле TF-IDF. Этот метод учитывает и частоту встречаемости слова в документе (TF) и обратную частоту документа (IDF).\n",
    "    Пример: Используется для извлечения ключевых признаков из текста, что полезно для задач, таких как классификация текстов, кластеризация и информационный поиск.\n",
    "\n",
    "* TF-IDF с N-граммами ('tf-idf_ngram'):\n",
    "\n",
    "    Описание: Этот режим также использует TF-IDF, но включает векторизацию не только отдельных слов, но и последовательностей из N смежных слов (N-грамм).\n",
    "    Применение: Учитывает контекстуальные зависимости между последовательными словами, что может улучшить качество представления текста, особенно для задач, где важен порядок слов, например, в задачах машинного перевода или анализе sql запросов.\n",
    "\n",
    "* Мешок слов ('bag_of_words'):\n",
    "\n",
    "    Описание: Это простой метод векторизации, который представляет каждый документ в виде вектора, где каждая компонента соответствует наличию или отсутствию конкретного слова из словаря.\n",
    "    Применение: Подходит для задач, где важно только наличие слова в тексте, а не его частота или важность. Обычно используется в моделях, требующих быстрое обучение и простоту интерпретации, таких как методы наивного Байеса или простые линейные модели.\n",
    "\n",
    "* Мешок символов ('bag_of_characters'):\n",
    "\n",
    "    Описание: Этот метод представляет текст в виде вектора, где каждая компонента соответствует наличию или отсутствию конкретного символа в тексте.\n",
    "    Применение: Полезен в задачах, где важны структура или последовательность символов, таких как распознавание рукописного текста или анализ кодов программ. \n",
    "\n",
    "#### Описание `FeatureSelecter`\n",
    "`FeatureSelecter` — класс, реализующий отбор ключевых токенов из общего набора. Отбор производится двумя комбинацией двух методов: SelectFromModel(Lasso) и TruncatedSVD.\n",
    "\n",
    "`fit_transform` — Обучает SelectFromModel и TruncatedSVD модели на тренировочных данных. Преобразует тренировочные данные с использованием обученных моделей. Возвращает преобразованные тренировочные данные.\n",
    "\n",
    "`transform` — выполняет только отбор токенов на основе работы предобученных моделей.\n",
    "\n",
    "\n",
    "#### Описание `ModelFabric` \n",
    "`ModelFabric` — класс, предоставляющий единый интерфейс для создания моделей классификации данных\n",
    "\n",
    "Поддерживает создание следующих моделей:\n",
    "* XGBOOST\n",
    "* SVC\n",
    "* NU_SVC\n",
    "* KNEIGHBORS\n",
    "* DECISION_TREE\n",
    "* RANDOM_FOREST\n",
    "* ADABOOST\n",
    "* BAGGING\n",
    "* EXTRA_TREES\n",
    "* LINEAR_SVC"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9fdd964-7987-4cb7-8ed7-cbcf7f84b367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:39.155948Z",
     "start_time": "2024-11-22T22:07:38.807163Z"
    }
   },
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "634820cd-d106-48da-8a8d-0a349b55f0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:45.712929Z",
     "start_time": "2024-11-22T22:07:45.705318Z"
    }
   },
   "source": [
    "class ExtractMethods:\n",
    "    TF_IDF = 'tf-idf'\n",
    "    TF_IDF_NGRAM = 'tf-idf_ngram'\n",
    "    BAG_OF_WORDS = 'bag_of_words'\n",
    "    BAG_OF_CHAR = 'bag_of_characters'\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, method, *args, **kwargs):\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.notes = {}\n",
    "        self.vectorizer = self._get_vectorizer(method, args, kwargs)\n",
    "\n",
    "    def _get_vectorizer(self, method, *args, **kwargs):\n",
    "        token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "        match method:\n",
    "            case ExtractMethods.TF_IDF:\n",
    "                return TfidfVectorizer(tokenizer=token.tokenize, *self.args, **self.kwargs)\n",
    "            case ExtractMethods.TF_IDF_NGRAM:\n",
    "                return TfidfVectorizer(\n",
    "                    lowercase=True, stop_words='english',\n",
    "                    ngram_range=(1, 3),\n",
    "                    tokenizer=token.tokenize, analyzer='char'\n",
    "                )\n",
    "            case ExtractMethods.BAG_OF_WORDS:\n",
    "                return CountVectorizer(analyzer='word', **self.kwargs)\n",
    "            case ExtractMethods.BAG_OF_CHAR:\n",
    "                return CountVectorizer(analyzer='char', tokenizer=token.tokenize, **self.kwargs)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unknown feature extraction method: {self.method}\")\n",
    "\n",
    "    def fit_extract(self, x_train):\n",
    "        return self.vectorizer.fit_transform(x_train)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.vectorizer.transform(x)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "494d0873-d166-4a60-8b43-71f8058ce824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:48.257416Z",
     "start_time": "2024-11-22T22:07:48.223192Z"
    }
   },
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bd25dcd1-e0bd-4e76-b4e4-32bda0e72ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:49.929703Z",
     "start_time": "2024-11-22T22:07:49.924005Z"
    }
   },
   "source": [
    "class FeatureSelecter:\n",
    "    def __init__(self, n_components):\n",
    "        self._scaler = StandardScaler(with_mean=False)\n",
    "        self._lasso_selecter = SelectFromModel(Lasso(alpha=0.001, random_state=10))\n",
    "        self._pca = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, x_train, y_train):\n",
    "        self._scaler.fit(x_train)\n",
    "        self._lasso_selecter.fit(self._scaler.transform(x_train), y_train)\n",
    "        self._pca.fit(self._lasso_selecter.transform(self._scaler.transform(x_train)))\n",
    "\n",
    "        return self._pca.transform(self._lasso_selecter.transform(self._scaler.transform(x_train)))\n",
    "\n",
    "    def transform(self, x):\n",
    "        return self._pca.transform(self._lasso_selecter.transform(self._scaler.transform(x)))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "ebbfdda3-38e7-4b26-81f1-c4e80c43c03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:51.388106Z",
     "start_time": "2024-11-22T22:07:51.333959Z"
    }
   },
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid, RadiusNeighborsClassifier\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1df89125-3df1-4133-a3ce-e6bb969ecec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:07:53.022601Z",
     "start_time": "2024-11-22T22:07:53.015201Z"
    }
   },
   "source": [
    "class Models:\n",
    "    XGBOOST = 'XGBoost'\n",
    "    SVC = 'SVC'\n",
    "    NU_SVC = 'NuSVC'\n",
    "    KNEIGHBORS = 'KNeighbors'\n",
    "    DECISION_TREE = 'DecisionTree'\n",
    "    RANDOM_FOREST = 'RandomForest'\n",
    "    ADABOOST = 'AdaBoost'\n",
    "    BAGGING = 'Bagging'\n",
    "    EXTRA_TREES = 'ExtraTrees'\n",
    "    LINEAR_SVC = 'LinearSVC'\n",
    "\n",
    "\n",
    "class ModelFabric:\n",
    "    @staticmethod\n",
    "    def create_model(model_name, *args, **kwargs):\n",
    "        match model_name:\n",
    "            case Models.XGBOOST:\n",
    "                return xgb.XGBClassifier(*args, **kwargs)\n",
    "            case Models.SVC:\n",
    "                return SVC(gamma=2, C=1, *args, **kwargs)\n",
    "            case Models.NU_SVC:\n",
    "                return NuSVC(*args, **kwargs)\n",
    "            case Models.KNEIGHBORS:\n",
    "                return KNeighborsClassifier(3, *args, **kwargs)\n",
    "            case Models.DECISION_TREE:\n",
    "                return DecisionTreeClassifier(max_depth=5, *args, **kwargs)\n",
    "            case Models.RANDOM_FOREST:\n",
    "                return RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, *args, **kwargs)\n",
    "            case Models.ADABOOST:\n",
    "                return AdaBoostClassifier(*args, **kwargs)\n",
    "            case Models.BAGGING:\n",
    "                return BaggingClassifier(*args, **kwargs)\n",
    "            case Models.EXTRA_TREES:\n",
    "                return ExtraTreesClassifier(*args, **kwargs)\n",
    "            case Models.LINEAR_SVC:\n",
    "                return LinearSVC(*args, **kwargs)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported model name: {model_name}\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "b40ea304-727d-4bd1-9c5f-937d6d4fde20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:09.456219Z",
     "start_time": "2024-11-22T22:08:09.452433Z"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_expect, y_pred):\n",
    "    cm = confusion_matrix(y_expect, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=['Normal', 'SQL-Injection'],\n",
    "                yticklabels=['Normal', 'SQL-Injection'])\n",
    "    plt.xlabel('Prediction', fontsize=13)\n",
    "    plt.ylabel('Actual', fontsize=13)\n",
    "    plt.title('Confusion Matrix', fontsize=17)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Загрузим исходный датасет `train-dataset.tsv` при помощи модуля `pandas` и разобьем его на обучающий и тестовый набор данных:\n",
    "    Метод train_test_split из библиотеки sklearn.model_selection используется для разделения данных на обучающую и тестовую выборки. Основная цель этого метода – разделить данные таким образом, чтобы часть данных использовалась для обучения модели, а другая часть – для тестирования эффективности этой модели на новых данных, которые модель ранее не видела.\n",
    "    Основные параметры метода train_test_split\n",
    "        * arrays – массивы данных, подлежащие разделению. Могут быть указаны как один массив, так и несколько массивов.\n",
    "        * test_size – доля данных, выделяемых для тестового набора. Значение должно находиться в диапазоне от 0 до 1, где 0.5 означает, что 50% данных будут использоваться для обучения, а оставшиеся 50% – для теста.\n",
    "        * random_state – целое число, задающее начальное состояние генератора псевдослучайных чисел. Это обеспечивает воспроизводимость результатов при многократном запуске программы.\n",
    "        * shuffle – логическое значение, определяющее, следует ли перемешивать данные перед разделением."
   ],
   "id": "9ca38c63298b5d59"
  },
  {
   "cell_type": "code",
   "id": "626f155b-cfe8-4f6e-ae7e-e26f03e7844a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:14.218731Z",
     "start_time": "2024-11-22T22:08:14.095961Z"
    }
   },
   "source": [
    "source_df = pd.read_csv(\"datasets/train-dataset.tsv\", sep='\\t', engine='python')\n",
    "df_train, df_test = train_test_split(source_df, test_size=0.5)\n",
    "# Выделим из выборки для обучения df_train столбец payload в вектор x_train, а столбц label в вектор y_train\n",
    "x_train, y_train = df_train['payload'].values, df_train['label'].values\n",
    "# Выделим из тестовой выборки df_test столбец payload в вектор x_test, а столбц label в вектор y_test\n",
    "x_test, y_test = df_test['payload'].values, df_test['label'].values\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(x_train)}, Размер тестовой выборки: {len(x_test)}\")\n",
    "\n",
    "test_df = pd.read_csv('datasets/test-dataset.csv')\n",
    "test_x = test_df['payload'].values\n",
    "test_y = test_df['label'].values\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 15304, Размер тестовой выборки: 15305\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Извлечь токенезировать и векторизировать данные представленные в `train-dataset.tsv` при помощи инструментов `nltk.tokenize.RegexpTokenizer`, `sklearn.feature_extraction.text.TfidfVectorizer`, `sklearn.feature_extraction.text.CountVectorizer`",
   "id": "3ef8909c1548f18d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1. Выбор метода векторизации.\n",
    "Выбор метода векторизации текста — важная задача, поскольку от правильного выбора метода зависят точность и эффективность последующих операций, таких как классификация, кластеризация и поиск информации. Рассмотрим основные методы векторизации и их преимущества.\n",
    "\n",
    "TF-IDF — один из самых популярных методов векторизации, который объединяет частоту появления термина в документе с обратной частотой его появления в корпусе документов. Этот метод считается мощным благодаря своему умению выявлять важные термины, которые редко встречаются в большинстве документов, но часто встречаются в одном документе.\n",
    "\n",
    "Метод TF-IDF_NGRAM — это расширение классического метода TF-IDF, которое включает в себя учет не только одиночных слов, но и последовательностей слов (n-grams). В отличие от обычного TF-IDF, который фокусируется исключительно на единицах лексического уровня (словах), TF-IDF_ngram добавляет дополнительный уровень внимания к последовательностям слов, что позволяет более точно учитывать контекст и смысл текста.\n",
    "\n",
    "BoW — метод, который рассматривает документы как мешанину слов, где каждый уникальный термин рассматривается как единица, вне зависимости от порядка и контекста. BoW просто подсчитывает количество вхождений каждого термина.\n",
    "\n",
    "Метод bag_of_characters — это один из подходов к векторизации текста, который фокусируется на уровне символов, а не слов. В отличие от более распространённых методов, таких как bag_of_words и TF-IDF, которые работают на уровне слов, bag_of_chars обрабатывает текст как последовательность символов.\n"
   ],
   "id": "3b288f5a94fbdf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:18.835814Z",
     "start_time": "2024-11-22T22:08:18.831992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attributes = dir(ExtractMethods)\n",
    "methods = [getattr(ExtractMethods, attr) for attr in attributes if\n",
    "           not callable(getattr(ExtractMethods, attr)) and not attr.startswith('__')]\n",
    "\n",
    "for method in methods:\n",
    "    print(method)"
   ],
   "id": "97faa53a404c2727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_characters\n",
      "bag_of_words\n",
      "tf-idf\n",
      "tf-idf_ngram\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 3.1.1. Инициализация экстрактора признаков для методов 'BAG_OF_CHAR', 'BAG_OF_WORDS', 'TF_IDF', 'TF_IDF_NGRAM'",
   "id": "7694c794a730add5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:21.935292Z",
     "start_time": "2024-11-22T22:08:21.931690Z"
    }
   },
   "cell_type": "code",
   "source": "extractors = {method: FeatureExtractor(method) for method in methods}",
   "id": "f3bdf64e4fd869d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 3.1.2. Фитинг и трансформация обучающих данных для методов 'BAG_OF_CHAR', 'BAG_OF_WORDS', 'TF_IDF', 'TF_IDF_NGRAM'.",
   "id": "ecc2e900ca6d924f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:25.421138Z",
     "start_time": "2024-11-22T22:08:23.835541Z"
    }
   },
   "cell_type": "code",
   "source": "x_train_vectors = {method: extractors[method].fit_extract(x_train) for method in methods}\n",
   "id": "3a5079956facde0d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 3.1.3. Трансформация тестовых данных",
   "id": "8e4085ce4de7f9f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:29.272852Z",
     "start_time": "2024-11-22T22:08:27.866750Z"
    }
   },
   "cell_type": "code",
   "source": "x_test_vectors = {method: extractors[method].extract_features(x_test) for method in methods}\n",
   "id": "14a93043a2c716f7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 3.1.4. Теперь у нас есть векторизованные данные, готовые для обучения модели",
   "id": "92ec1519e4d6d98c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:34.185238Z",
     "start_time": "2024-11-22T22:08:34.181114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for method in methods:\n",
    "    print(method, (int)(1 * (x_train_vectors[method].shape[0])), (int)(1 * (x_train_vectors[method].shape[1])),\n",
    "          (int)(0.10 * (x_train_vectors[method].shape[1])))"
   ],
   "id": "f1c119c3ed82998f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_characters 15304 75 7\n",
      "bag_of_words 15304 14733 1473\n",
      "tf-idf 15304 14763 1476\n",
      "tf-idf_ngram 15304 22603 2260\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Выбрать ключевые, наиболее значемые токены при помощи инструментов `sklearn.feature_selection.SelectFromModel` и `sklearn.decomposition.TruncatedSVD`",
   "id": "8901a3622fd0d670"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4.1 Инициализация объекта класса FeatureSelecter\n",
    "Класс FeatureSelecter предназначен для выбора ключевых, наиболее значимых токенов с использованием методов Lasso и TruncatedSVD. Давайте пошагово разберём, как он работает и как его можно применить для получения значимых токенов.\n",
    "Шаги работы класса FeatureSelecter\n",
    "Стандартизация данных:\n",
    "    * Класс использует StandardScaler для стандартизации данных. Стандартизация необходима для приведения всех признаков к одинаковым масштабам, что улучшает работу алгоритмов машинного обучения.\n",
    "    * Отбор признаков с использованием Lasso: Алгоритм Lasso используется для отбора признаков методом регуляризации. Регуляризация Lasso способствует тому, что многие коэффициенты становятся равны нулю, тем самым исключая малозначимые признаки.\n",
    "    * Уменьшение размерности с использованием TruncatedSVD: TruncatedSVD применяется для уменьшения размерности данных, сохраняя при этом важные аспекты вариации в данных. Это помогает избавиться от шума и упростить дальнейшую обработку данных."
   ],
   "id": "796df57b866b80a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Параметр n_components контролирует количество компонентов (измерений) в новом пространстве после преобразования. В частности, он отвечает за количество компонентов в результирующем пространстве после применения метода.\n",
    "Контроль над этим параметром необходим для управления качеством сжатия данных. Чем меньше компонентов, тем сильнее сжимается пространство, но при этом увеличивается риск потери важной информации. Чем больше компонентов, тем точнее сохраняется информация, но увеличивается вычислительная сложность и затраты памяти."
   ],
   "id": "a889af2f7659ada2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:40.788497Z",
     "start_time": "2024-11-22T22:08:40.781466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "method_components = {method: (int)(0.05 * (x_train_vectors[method].shape[1])) for method in methods}\n",
    "method_components"
   ],
   "id": "7feccba3234c3d5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bag_of_characters': 3,\n",
       " 'bag_of_words': 736,\n",
       " 'tf-idf': 738,\n",
       " 'tf-idf_ngram': 1130}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:08:46.438583Z",
     "start_time": "2024-11-22T22:08:46.435100Z"
    }
   },
   "cell_type": "code",
   "source": "selectors = {method: FeatureSelecter(n_components=method_components[method]) for method in method_components.keys()}",
   "id": "61335ece9c0aaa30",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.2 Применение fit_transform к обучающим данным\n",
   "id": "2634255567515d1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:23:53.361354Z",
     "start_time": "2024-11-22T22:08:49.130471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_selected_by_method_vectorize = {}\n",
    "for method in methods:\n",
    "    print(f\"{time.time()} Применение fit_transform к обучающим данным в разрезе медода векторизации: {method}\")\n",
    "    x_train_selected_by_method_vectorize[method] = selectors[method].fit_transform(x_train_vectors[method], y_train)"
   ],
   "id": "f8f8db284e35f094",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732313329.1328604 Применение fit_transform к обучающим данным в разрезе медода векторизации: bag_of_characters\n",
      "1732313330.1208408 Применение fit_transform к обучающим данным в разрезе медода векторизации: bag_of_words\n",
      "1732313613.4777737 Применение fit_transform к обучающим данным в разрезе медода векторизации: tf-idf\n",
      "1732313803.582404 Применение fit_transform к обучающим данным в разрезе медода векторизации: tf-idf_ngram\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:24:14.325291Z",
     "start_time": "2024-11-22T22:24:14.320933Z"
    }
   },
   "cell_type": "code",
   "source": "x_train_selected_by_method_vectorize['tf-idf']\n",
   "id": "26880bf63645de02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.81655995e+00, -2.32265680e+00, -1.75700403e-02, ...,\n",
       "         7.19742233e-02,  1.42582098e-01, -1.64947802e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 4.15273844e-02, -4.83715434e-02, -2.69978860e-03, ...,\n",
       "        -9.36480866e-02,  8.36290372e-01, -4.57110285e-01],\n",
       "       [ 9.89476737e-01, -1.08485534e+00,  5.03489629e-03, ...,\n",
       "        -1.40385562e+00,  2.74549579e+00, -3.13848871e+00],\n",
       "       [ 1.21068428e+00, -1.95609543e+00, -5.69669575e-02, ...,\n",
       "        -7.97745392e-02,  9.85807737e-01, -7.62880966e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.3 Применение transform к тестовым данным\n",
   "id": "c513be24b2946a2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:24:24.500972Z",
     "start_time": "2024-11-22T22:24:24.450755Z"
    }
   },
   "cell_type": "code",
   "source": "x_test_selected_tf_idf = selectors['tf-idf'].transform(x_test_vectors['tf-idf'])\n",
   "id": "4ec55daed4a33a2c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T22:24:27.207724Z",
     "start_time": "2024-11-22T22:24:27.203474Z"
    }
   },
   "cell_type": "code",
   "source": "x_test_selected_tf_idf",
   "id": "c6c23fe9e9d853f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.98193937, -2.67339022, -0.07126581, ..., -0.72287046,\n",
       "        -0.11252313,  0.08979638],\n",
       "       [ 8.38043389, -2.00088664, -0.09698556, ..., -0.22763743,\n",
       "         0.60739055,  0.36080972],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 2.10052548, -3.23683363, -0.09859041, ...,  0.6191329 ,\n",
       "        -0.1729047 , -0.19087283],\n",
       "       [ 1.17211125, -1.33254097,  0.02778814, ..., -0.00838185,\n",
       "        -0.07681123,  0.50793046],\n",
       "       [ 7.54047194,  4.8225702 ,  0.03570947, ..., -0.14966879,\n",
       "         0.30592337,  0.02896673]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.4. Размерность отобранных признаков",
   "id": "7d719de3b1618071"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T18:19:51.070843Z",
     "start_time": "2024-11-22T18:19:51.067070Z"
    }
   },
   "cell_type": "code",
   "source": "print(x_train_selected_by_method_vectorize['tf-idf'].shape)",
   "id": "6ebba4cd6a3e98a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15304, 10)\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Обучить модель на выбранных ключевых признаках",
   "id": "9c46fe8e22696dc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5.1 XGBoost — это библиотека для градиентного бустинга деревьев решений. Она известна своей скоростью и точностью, особенно в соревнованиях по машинному обучению.",
   "id": "3b1e34a414b7772a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff721b00543cb4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5.1 RandomForest — это ансамбль деревьев решений, который строит множество деревьев и усредняет результаты.",
   "id": "f535b45dea634c0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T14:25:04.132620Z",
     "start_time": "2024-11-21T14:25:04.088380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = Models.RANDOM_FOREST\n",
    "model = ModelFabric.create_model(model_name)\n",
    "model.fit(x_train_vectors, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ],
   "id": "1d4e3aec87a4ae99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6231\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a15d2020b22659f0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
